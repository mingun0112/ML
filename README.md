# Sklearn, Numpy, Pandas, Matplotlib
## Machine Learning
-------------------------------------------
### 1. K-Nearest Neighbor
![Scikit-learn](https://img.shields.io/badge/scikitlearn-F7931E.svg?&style=for-the-badge&logo=scikitlearn&logoColor=blue)
##### k-Nearest Neighbor(k-최근접 이웃, K-NN)알고리즘
###### Goal: 2개의 물고기 종류를 분류하는 머신러닝 모델을 훈련한다.
데이터 출처 [fish_maket](http://www.kaggle.com/aungpyaeap/fish-market)

---------------------------------
### 1-2. Supervised Learning
![Numpy](https://img.shields.io/badge/numpy-013243.svg?&style=for-the-badge&logo=numpy&logoColor=white)
##### Train set, Test set
###### Goal: Numpy를 이용해 1. BreamAndSmelt의 데이터셋을 Train과 Test로 나누어 학습을 진행한다.

----------------------------------

### 1-3. Preprocessing Data
##### Scale
###### Goal: 예측 결과가 잘못된 원인을 파악해보고 Scale 조절을 통해 문제를 해결해본다.

-------------------------------------
### 2. K_Nearest_Neighbor_Regression
![Scikit-learn](https://img.shields.io/badge/scikitlearn-F7931E.svg?&style=for-the-badge&logo=scikitlearn&logoColor=blue)
##### K_Nearest_Neighbor_Regression K 최근접 이웃 회귀
###### Goal: 회귀와 분류의 차이를 생각해보고 Overfitting과 Underfitting에 대해 배운다.

-----------------------------------

### 2-1. Linear_Regression
##### KNeighborsRegressor
###### Goal: K_Nearest_Neighbor_Regression의 문제를 확인하고 선형 회귀에 대해 배워 값을 예측한다.

--------------------------------

### 2-2. Feature_engineering
![Pandas](https://img.shields.io/badge/pandas-150458.svg?&style=for-the-badge&logo=pandas&logoColor=white)
##### Multiple Regression, Ridge, Lasso
###### Goal: Pandas를 이용해 더 많은 Feature들을 불러오고 다룰 수 있는 다중 회귀의 몇 가지 방법을 학습한다.

--------------------------------
### 3. Logistic Regression
##### Probability of KNN Classifier Prediction & Logistic Regression
###### Goal: Logistic Regression과 Softmax, Sigmoid 함수를 활용해 Multiple Classification을 수행한다.

---------------------------------
### 3-1. Gradient Descent
#####
